{"cells":[{"cell_type":"markdown","source":[" # Bernoulli RBM\n"," This notebook will show my implementation of the BernoulliRBM. I devided everything up into dedicated functions\n"," and I will try to reference Hinton's [A Practical Guide to Training Restricted Boltzmann Machines](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf).\n"," I was also inspired by code and blogs from following ressources:\n"," - [Amir Ali's comprehensive article](https://medium.com/machine-learning-researcher/boltzmann-machine-c2ce76d94da5)\n"," - [Luke Sun's very helpful Blog article](https://towardsdatascience.com/restricted-boltzmann-machine-as-a-recommendation-system-for-movie-review-part-2-9a6cab91d85b)\n"," - [Yusugomori's implementation](https://gist.github.com/yusugomori/4428308)\n"," - [Echen's implementation](https://github.com/echen/restricted-boltzmann-machines/blob/master/rbm.py)\n","\n"," For everyone lacking the basic understanding of RBM's I advice strongly to checkout Amir Ali's blog\n"," and for the implementation Luke Sun's article. The code examples help to understand how to properly structure the code.\n"," Both are fairly different because there are a lot of slightly different implementations due to the design choices that one can make according to Hinton.\n",""],"metadata":{}},{"cell_type":"markdown","source":[" ## The dataset\n"," I am starting with the data I want to train on. It's quite simple and just 6 binary values per data point.\n"," What's important here, is to notice that the 3rd and the 6th value for each data point is 1 and 0 respectively.\n"," Why is that important? Because that means that the reconstruction should always reconstruct samples that show the same trait.\n"," View it as some \"binary-code\" which we corrupt in the test data and that needs to be reconstructed again.\n"," Another example would be an image with a whole which we impaint upon reconstruction."],"metadata":{}},{"cell_type":"code","execution_count":1,"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","data = np.array([[1, 1, 1, 0, 0, 0], [1, 0, 1, 0, 0, 0], [1, 1, 1, 0, 0, 0], [0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 0, 0],\n","                 [0, 0, 1, 1, 1, 0], [0, 0, 1, 0, 1, 0], [1, 0, 1, 0, 0, 0], [0, 0, 1, 1, 0, 0]])\n","data"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 1, 1, 0, 0, 0],\n","       [1, 0, 1, 0, 0, 0],\n","       [1, 1, 1, 0, 0, 0],\n","       [0, 0, 1, 1, 1, 0],\n","       [0, 0, 1, 1, 0, 0],\n","       [0, 0, 1, 1, 1, 0],\n","       [0, 0, 1, 0, 1, 0],\n","       [1, 0, 1, 0, 0, 0],\n","       [0, 0, 1, 1, 0, 0]])"]},"metadata":{},"execution_count":1}],"metadata":{}},{"cell_type":"markdown","source":[" ## The hyperparameters\n"," Next I define some structural parameters of the RBM like the number of visible and hidden nodes.\n"," I also define hyper parameters learning rate, number of training epochs and sampling rounds *k*.\n"," k is important for the gibbs-sampling."],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["n_visible = data.shape[1]\n","n_hidden = 2\n","num_examples = data.shape[0]\n","lr = 1\n","epochs = 2000\n","k = 2"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ## Initializing the parameters of the model\n"," I wrote a function to initialize the weight matrix W, the visible bias vector v and the hidden bias vector h."],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["def init_parameters(n_visible, n_hidden):\n","    weights = np.asarray(\n","        np.random.uniform(low=-0.1 * np.sqrt(6. / (n_hidden + n_visible)),\n","                          high=0.1 * np.sqrt(6. / (n_hidden + n_visible)),\n","                          size=(n_visible, n_hidden)))\n","    hbias = np.random.randn(n_hidden)\n","    vbias = np.random.randn(n_visible)\n","    return weights, vbias, hbias"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ## Base functions\n"," The sigmoid function is the non-linearity for each node."],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["def sigmoid(x):\n","    return (1 / (1 + np.exp(-x)))"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" We can use the sigmoid to now compute the probabilities\n","\n"," [](./res/sample_ph)\n","\n"," and\n","\n"," [](./res/sample_pv)\n",""],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["def ph_given_v(v, W, hbias):\n","    return sigmoid((v @ W) + hbias)\n","\n","\n","def pv_given_h(h, W, vbias):\n","    return sigmoid((h @ W.T) + vbias)"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" Now it becomes necessary to use these probabilities to sample new versions of h as well as v.\n"," The simplest and most stable way is to use Bernoulli sampling. But Hinton also describes other possible sampling methods.\n"," An easy way is to generate uniformly distributed random numbers and every time they are lower than the respective element in the probability matrix you say it's a 1.\n"," Or... you just use a predefined numpy function which does exactly that for you.\n","\n"," **Extra:** Maybe one additional point on why the hidden states need to be binary as well.\n"," There's no real reason other than it helps generalizing strongly and is very very stable - hence, easy to train and code.\n"," You can in theory sample other things like multiple binaries, real numbers or discrete numbers but that quickly becomes more complicated.\n"," If we were to pass the probabilities directly back into the network we don't add any probablistic component to our model.\n",""],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["def sample_h_given_v(v, W, hbias):\n","    # It is very important to make these hidden states binary, rather than using the probabilities themselves.\n","    h_prob = ph_given_v(v, W, hbias)\n","    h_sampled = np.random.binomial(\n","        size=h_prob.shape,  # discrete: binomial\n","        n=1,\n","        p=h_prob)\n","    return [h_prob, h_sampled]\n","\n","\n","def sample_v_given_h(h, W, vbias):\n","    # It is common to use the probability, pi, instead of sampling a binary value.\n","    # This is not nearly as problematic as using probabilities for the data-driven hidden states\n","    # and it reduces sampling noise thus allowing faster learning.\n","    # There is some evidence that it leads to slightly worse density models.\n","    v_prob = pv_given_h(h, W, vbias)\n","    v_sampled = np.random.binomial(\n","        size=v_prob.shape,  # discrete: binomial\n","        n=1,\n","        p=v_prob)\n","\n","    return [v_prob, v_sampled]"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ## How the contrastive divergence works\n"," I've long been thinking how to approach the next section which will talk about contrastive divergence.\n"," It's by far the most complicated concept.\n","\n"," Having discussed some of the more fundamental functions, I will now take a more top-down approach to explaining the next few functions.\n"," Starting with the training loop. Essentially consisting of all the structural parameters.\n"," Per epoch we will run one contrastive divergence step. An epoch can be further devided into mini-batches, if necessary.\n",""],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["def run_train_loop(data, W, vbias, hbias, epochs=1000, lr=1, k=10):\n","    for epoch in range(epochs):\n","        W, vbias, hbias = contrastive_divergence(data, W, vbias, hbias, lr, k)\n","        cost = compute_reconstruction_cross_entropy(data, W, vbias, hbias)\n","        if (epoch % (epochs // 10)) == 0:\n","            print(f'Training epoch {epoch}, cost is {cost}')\n","    return W, vbias, hbias"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ### Contrastive Divergences\n"," How does the contrastive step look like?\n"," It's quite simple: First you gibbs sample, then you train. Let's have a close look, though.\n",""],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["def contrastive_divergence(data, W, vbias, hbias, lr=0.1, k=1):\n","    pv_0, ph_0, pv_k, ph_k = gibb_sample(data, W, vbias, hbias, k)\n","    W, vbias, hbias = train_params(W, vbias, hbias, lr, pv_0, ph_0, pv_k, ph_k)\n","    return W, vbias, hbias"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ### Gibbs sampling\n"," In order to approximate the gradient we need to have the expectation of the joint probability of the data ($<h,v>_{data}$)and one for the model ($<h,v>_{model}$).\n"," For the model we could start with a random v state of binary values ([1,0,1,0,1,0]) and run it through the network until nothing changes anymore.\n"," Which means the energy is quite low. OR, we use gibbs sampling. Gibb's sampling entails two key ideas.\n"," First, we start from a given high-dimensional example $x \\in R^d$ of the data. Second, we use conditional probabilities to sample the next dimensions for the next data point using what we know about the example and the distribution as a whole.\n","\n"," The second part is quite unimportant for our case.\n"," Why? Because every node in v is independent from all other v and the same holds for h.\n"," And each node represent one dimension in our example $x$. Hence, we don't need nasty conditionals to sample.\n"," We only require the probabilities that are given to us by a forward pass or backward pass through the model.\n","\n"," That makes the *first* part quite important for this gibbs sampling approach.\n"," We choose a data point and pass it through the network in order to get probablities.\n"," We can use these probabilities with Bernoulli sampling as it is the most simple way to probablistically decide whether a dimension fires or not.\n"," And we can theoretically do this an infinite amount of times to get the $<h,v>_{model}$,\n"," but we do it just k-times to just get an approximation, which we call $<h_k,v_k>_{recon}$.\n","\n"," This is what you see in the implementation. $p(v_0)$ is our data which we use to compute $p(h_0)$ but also sample $h_0$ which is a binary vector.\n"," Now for k times we use v_k and h_k to retrieve our $<h_k,v_k>_{recon}$.\n"," we return the initial and new values.\n","\n","\n"," **EXTRA:** Not sure if I've should have mentioned that the code uses a data matrix not single data points.\n"," Hence, my explanation might talk about individual data points but in the implementation I am doing things in bulk."],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["def gibb_sample(data, W, vbias, hbias, k):\n","    pv_0, v_0 = data, data\n","    ph_0, h_0 = sample_h_given_v(pv_0, W, hbias)\n","    h_k = h_0\n","    for _ in range(k):\n","        # For the last update of the hidden units, it is silly to use stochastic binary states because nothing depends on which state is chosen.\n","        # So use the probability itself to avoid unnecessary sampling noise.\n","        # When using CDn, only the final update of the hidden units should use the probability\n","        pv_k, v_k = sample_v_given_h(h_k, W, vbias)\n","        ph_k, h_k = sample_h_given_v(v_k, W, hbias)\n","    return pv_0, ph_0, pv_k, ph_k"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ### Parameter update\n"," Uff, now onto the second complicated part. The training.\n"," You've probably seen two formulas:\n"," - [](./res/energy_function.png)\n","\n"," and\n","\n"," - [](./res/weight_update.png)\n","\n"," To be honest, I don't entirely get how they come to this result but most examples online don't attempt to explain why.\n"," The first is quite reasonable but nowhere found in the implementation. But that's fine. We need the derivative.\n","\n"," Hence, we have to compute $<v_i, h_j>$. Meaning we need the expectation of the joint probability of these values.\n"," We get the joint probability by computing the probability of a visible nodes activation $p(v=1)$ with the probability of a hidden nodes activation $p(h=1)$.\n"," However, as each hidden node depends on multiple visible nodes we have to sum these probabilities.\n"," This is done automatically by the network but this is why we often see p(H_j=1|v) in some algorithmic descriptions. (Or so I believe...)\n","\n"," **EXTRA:** I know this is kinda confusing and it was for me, too. However, you can think of it differently.\n"," Assume you have 2 v-nodes and 2 h-nodes, then what we want is essentially a table like this\n","\n"," |       | p(h1\\|v) | p(h2\\|v) |\n"," |-------|----------|----------|\n"," | **p(v1)** | p(v1,h1) | p(v1,h2) |\n"," | **p(v2)** | p(v2,h1) | p(v2,h2) |\n","\n"," This table is equivalent with multiplying each dimension of v with each dimension of h.\n"," Great these are probabilites. How do we get the expectations?\n"," Well, this is kinda already the expectation value we need. It's quite complicated to explain why that is, though.\n"," The expectation formula would require us to look into each configuration [(0,0), (0,1), (1,0) and (1,1)] and compute their probabilities and sum them up in a fairly complicated way.\n"," However, I think because we are just interested in one configuration we can skip these complicated parts.\n","\n"," Now back to business: With matrix computations we can greatly speed up the computation using the outer porduct of v and h.\n"," If we do things in bulk, we also have to devide buy the amount of data.\n","\n"," The rules for the bias terms follow a similar pattern of reasoning."],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["def train_params(W, vbias, hbias, lr, pv_0, ph_0, pv_k, ph_k):\n","    num_examples = pv_0.shape[0]\n","    # Expectation of (pi x pj) data where pi is the visible unit probability and pj the hidden unit probability (Could also be binary hence E[pi, hj]).\n","    # Outer product to get each h and v node combination multiplied\n","    pvh_data = (pv_0.T @ ph_0)\n","    pvh_recs = (pv_k.T @ ph_k)\n","\n","    # Division to get expected joint probability of all notes. In Hinton's words: average, per-case gradient computed on a mini-batch\n","    joint_p_vh = (pvh_data - pvh_recs) / num_examples\n","    W += lr * (joint_p_vh)\n","    vbias += lr * ((pv_0 - pv_k).sum(axis=0) / num_examples)\n","    hbias += lr * ((ph_0 - ph_k).sum(axis=0) / num_examples)\n","\n","    return W, vbias, hbias"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ### How to compute the loss\n"," Lastly, it becomes important to also compute the loss itself to see how well the learning works.\n"," It's a pretty simple binary cross entropy loss.\n",""],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["def compute_reconstruction_cross_entropy(v0, W, vbias, hbias):\n","    ph = ph_given_v(v0, W, hbias)\n","    pv = pv_given_h(ph, W, vbias)\n","\n","    binary_cross_entropy = -np.mean(np.sum(v0 * np.log(pv) + (1 - v0) * np.log(1 - pv)))\n","\n","    return binary_cross_entropy"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ## Running the code to train the model\n"," Putting everything together we only have to run our functions."],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["n_visible = data.shape[1]\n","n_hidden = 2\n","num_examples = data.shape[0]\n","lr = 1\n","epochs = 2000\n","k = 2\n","weights, vbias, hbias = init_parameters(n_visible, n_hidden)\n","\n","trained_weights, trained_vbias, trained_hbias = run_train_loop(data, weights, vbias, hbias, epochs=epochs, lr=lr, k=k)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch 0, cost is 31.28266976665344\n","Training epoch 200, cost is 8.712436715094332\n","Training epoch 400, cost is 8.619591539956083\n","Training epoch 600, cost is 8.602109827471823\n","Training epoch 800, cost is 8.70301327344577\n","Training epoch 1000, cost is 8.620467881804023\n","Training epoch 1200, cost is 8.618283214991504\n","Training epoch 1400, cost is 8.60993142994843\n","Training epoch 1600, cost is 8.610280376450985\n","Training epoch 1800, cost is 8.621156068468276\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" ## How to reconstruct values\n"," A simple reconstruction becomes trivial."],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["def reconstruct(v, W, vbias, hbias):\n","    ph, h = sample_h_given_v(v, W, hbias)\n","    pv, v = sample_v_given_h(h, W, vbias)\n","    reconstructed = pv\n","    return reconstructed"],"outputs":[],"metadata":{}},{"cell_type":"markdown","source":[" ## Testing the model\n"," Lastly, we want to test our model.\n"," We just constuct some additional examples and run them through the reconstruction function."],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["v = np.array([[0, 0, 0, 1, 1, 0], [1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 1], [0, 0, 1, 0, 1, 0]])\n","print(\"Test:\")\n","print(v)\n","print(\"Reconstruction:\")\n","print(reconstruct(v, trained_weights, trained_vbias, trained_hbias).round(decimals=2))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Test:\n","[[0 0 0 1 1 0]\n"," [1 1 0 0 0 0]\n"," [1 1 0 0 0 1]\n"," [0 0 1 0 1 0]]\n","Reconstruction:\n","[[0.  0.  1.  0.8 0.6 0. ]\n"," [1.  0.5 1.  0.  0.  0. ]\n"," [1.  0.5 1.  0.  0.  0. ]\n"," [0.  0.  1.  0.8 0.6 0. ]]\n"]}],"metadata":{}},{"cell_type":"markdown","source":[" ## Famous last words\n"," You should see how the 3 and 6 value are indeed reconstructed.\n"," For the remaining values we could in theory Bernoulli-sample them again to produce a new data point from the joint distribution that we learned.\n"," That's all there is to know. I also have variants of this computation, but they are implemented in an object oriented way.\n"," One of them computes the same RBM with the MNIST dataset."],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}